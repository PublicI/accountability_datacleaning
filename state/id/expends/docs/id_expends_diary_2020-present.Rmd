---
title: "Idaho Expenditures, 2020-present"
author: "Kiernan Nicholls & Yanqi Xu"
date: "`r Sys.time()`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 2
editor_options: 
  chunk_output_type: console
---

<!-- Place comments regarding knitting here -->

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  collapse = TRUE,
  comment = "#>",
  fig.path = "../plots/",
  fig.width = 10,
  dpi = 300
)
if (!interactive()) {
  options(width = 99)
  set.seed(5)
}
```

```{r create_docs_dir, eval=FALSE, echo=FALSE, include=FALSE}
fs::dir_create(here::here("id", "expends", "docs"))
```

## Project

The Accountability Project is an effort to cut across data silos and give
journalists, policy professionals, activists, and the public at large a simple
way to search across huge volumes of public data about people and organizations.

Our goal is to standardizing public data on a few key fields by thinking of each
dataset row as a transaction. For each transaction there should be (at least) 3
variables:

1. All **parties** to a transaction
2. The **date** of the transaction
3. The **amount** of money involved

## Objectives

This document describes the process used to complete the following objectives:

1. How many records are in the database?
1. Check for duplicates
1. Check ranges
1. Is there anything blank or missing?
1. Check for consistency issues
1. Create a five-digit ZIP Code called `zip`
1. Create a `year` field from the transaction date
1. Make sure there is data on both parties to a transaction

## Packages

The following packages are needed to collect, manipulate, visualize, analyze,
and communicate these results. The `pacman` package will facilitate their
installation and attachment.

The IRW's `campfin` package will also have to be installed from GitHub. This
package contains functions custom made to help facilitate the processing of
campaign finance data.

```{r load_packages, message=FALSE, warning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_gh("irworkshop/campfin")
pacman::p_load(
  tidyverse, # data manipulation
  snakecase, # column naming
  lubridate, # datetime strings
  jsonlite, # from json data
  magrittr, # pipe opperators
  gluedown, # printing markdown
  janitor, # dataframe clean
  aws.s3, # upload to aws s3
  refinr, # cluster and merge
  scales, # format strings
  readxl, # read excel files
  knitr, # knit documents
  rvest, # read html pages
  vroom, # read files fast
  glue, # combine strings
  httr, # http requests
  here, # relative storage
  fs # search storage 
)
```

This document should be run as part of the `R_campfin` project, which lives as a
sub-directory of the more general, language-agnostic
[`irworkshop/accountability_datacleaning`][repo] GitHub repository.

The `R_campfin` project uses the [RStudio projects][rproj] feature and should be
run as such. The project also uses the dynamic `here::here()` tool for file
paths relative to _your_ machine.

```{r where_here}
# where does this document knit?
here::here()
```

[repo]: https://github.com/irworkshop/accountability_datacleaning
[rproj]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects

## Data

The data was provided as a public records request and was fulfilled on Dec. 4, 2023. For data prior to 2020, see this [data dictionary](https://github.com/irworkshop/accountability_datacleaning/blob/8689889cefa62b7e5b0bdad5133123ed72fe9393/state/id/contribs/docs/id_contribs_diary.md?plain=1#L827).

```{r download_raw}
raw_dir <- dir_create(here("state","id", "expends", "data", "raw"))
```


### Read

Each file has a different column order and names. We will first use 
`purrr::map()` to use `readxl::read_excel()` and create a list of data frames.

```{r read_csv}
ide <- read_csv(
  dir_ls(raw_dir)
) %>% clean_names()
```

In this update, since we would like to create a new dataset for records from 2020-01-01 and on, we need to remove records after 2020-01-01 in the last update. We will do so by segregating these 2020 records and replace the data in the previous update.
```{r}
prev_dir <- here("state","id", "expends", "data", "previous")

ide_prev <- read_csv(path(prev_dir, "id_expends_19990101-20220309.csv"))

x <- ide_prev %>% filter(date>=as.Date("2020-01-01") & (source_file == "get_activity.json"))

ide_prev_new <- anti_join(ide_prev, x)

ide_prev_new %>% write_csv(path(prev_dir, "id_expends_1999-2019.csv"))
```

```{r}
count(ide,donate_type)
```

```{r dates}
count_na(ide$date)
```
## Explore

```{r glimpse}
glimpse(ide)
head(ide)
tail(ide)
```

We should first identify which columns are missing the kinds of key information
we need to properly identify all parties to a contribution. We can do this
with `campfin::flag_na()` after creating a new

```{r count_na}
col_stats(ide, count_na)
```

```{r flag_na}
ide <- ide %>% 
  flag_na(from_name, to_name, date, amount)

sum(ide$na_flag)
mean(ide$na_flag)
```


Records that are entirely duplicated at least once across all columns should
also be identified with `campfin::flag_dupes()`. The first occurrence of the
record is not flagged, but all subsequent duplicates are. 

Upon running the algorithm, we found no completely identical rows. 

```{r flag_dupes}
ide <- flag_dupes(ide, everything(), .check = TRUE)
```

### Categorical

```{r n_distinct}
col_stats(ide, n_distinct)
```

```{r bar_office, echo=FALSE}
explore_plot(
  data = ide,
  var = from_office
) + 
  theme(axis.text.x = element_text(angle = 10, vjust = 0.7)) +
  labs(
    title = "Idaho Expenditures by Office, 2020-2023",
    caption = "Source: Idaho Elections Office"
  )
```

```{r bar_party, echo=FALSE}
explore_plot(
  data = ide,
  var = from_party
) +
  labs(
  title = "Idaho Expenditures by Office, 2020-2023",
  caption = "Source: Idaho Elections Office"
  )
```


### Amounts

```{r amount_summary}
summary(ide$amount)
mean(ide$amount <= 0, na.rm = TRUE)
```

```{r amount_minmax}
glimpse(ide[c(which.min(ide$amount), which.max(ide$amount)), ])
```

```{r amount_histogram, echo=FALSE}
ide %>%
  filter(amount > 1) %>% 
  ggplot(aes(amount)) +
  geom_histogram(fill = dark2["purple"], bins = 20) +
  scale_y_continuous(labels = comma) +
  scale_x_continuous(
    breaks = c(1 %o% 10^(0:6)),
    labels = dollar,
    trans = "log10"
  ) +
  labs(
    title = "Idaho Expenditures Amount Distribution",
    subtitle = "from 2020 to 2023",
    caption = "Source: Idaho Elections Office",
    x = "Amount",
    y = "Count"
  )
```

```{r amount_violin, echo=FALSE}
ide %>%
  filter(
    amount > 1,
    amount < 1e6,
  ) %>% 
  ggplot(aes(x = from_party, y = amount)) +
  geom_violin(aes(fill = from_party), adjust = 2) +
  scale_fill_brewer(palette = "Dark2", guide = FALSE) +
  scale_y_continuous(
    breaks = c(1 %o% 10^(0:6)),
    labels = dollar,
    trans = "log10"
  ) +
  labs(
    title = "Idaho Expenditures Amount Distribution",
    subtitle = "from 2020 to 2023, by Party",
    caption = "Source: Idaho Elections Office",
    x = "Amount",
    y = "Count"
  )
```

### Dates

```{r year_add}
ide <- mutate(ide, year = year(date))
```

```{r date_range}
min(ide$date, na.rm = TRUE)
max(ide$date, na.rm = TRUE)
ide <- mutate(ide, date_flag = date > today() | year < 1999 | is.na(date))
count_na(ide$date) 
sum(ide$date_flag) 
mean(ide$date_flag)

ide <- ide %>% select(-date_flag)
```

```{r}
ide %>% 
  count(year) %>% 
  mutate(even = is_even(year)) %>% 
  ggplot(aes(x = year, y = n)) +
  geom_col(aes(fill = even)) + 
  scale_fill_brewer(palette = "Dark2") +
  scale_y_continuous(labels = comma) +
  scale_x_continuous(breaks = seq(2020, 2023, by = 2)) +
  theme(legend.position = "bottom") +
  labs(
    title = "Idaho Expenditures by Year",
    caption = "Source: Idaho Elections Office",
    fill = "Election Year",
    x = "Year Made",
    y = "Count"
  )
```

## Wrangle

### Address

```{r address_norm}
ide <- ide %>% 
  mutate_at(.vars = vars(ends_with('address')),
            .funs = list(norm = ~ normal_address(address = .,
      abbs = usps_city,
      na_rep = TRUE)))
```

```{r address_view}
ide %>% 
  select(contains("address")) %>% 
  distinct() %>% 
  sample_frac()
```

### ZIP

```{r zip_norm}
ide %>% select(ends_with('zip')) %>% 
  map_dbl(prop_in, valid_zip, na.rm = TRUE) %>% map_chr(percent) %>% glimpse()

ide <- ide %>% 
  mutate_at(.vars = vars(ends_with('zip')), .funs = list(norm = ~ normal_zip(.)))

ide %>% select(contains('zip')) %>% 
  map_dbl(prop_in, valid_zip, na.rm = TRUE) %>%  map_chr(percent)
```

```{r zip_progress}
progress_table(
  ide$to_zip,
  ide$to_zip_norm,
  ide$from_zip,
  ide$from_zip_norm,
  compare = valid_zip
)
```

### State

```{r state_norm}
ide %>% select(ends_with('state')) %>% 
  map_dbl(prop_in, valid_state, na.rm = TRUE) %>% map_chr(percent) %>% glimpse()

ide <- ide %>% mutate(from_state_norm = normal_state(from_state))
```

```{r state_progress}
progress_table(
  ide$from_state,
  ide$from_state_norm,
  compare = valid_state
)
```

### City

```{r city_norm}
ide %>% select(ends_with('city')) %>% 
  map_dbl(prop_in, valid_city, na.rm = TRUE) %>% map_chr(percent) %>% glimpse()

valid_place <- c(valid_city,extra_city)

ide <- ide %>% 
  mutate_at(.vars = vars(ends_with('city')),
            .funs = list(norm = ~ normal_city(city = .,abbs = usps_city,
                                            states = c(valid_state),
                                            na = invalid_city,
                                            na_rep = TRUE)))
ide %>% select(contains('city')) %>% 
  map_dbl(prop_in, valid_place, na.rm = TRUE) %>%  map_chr(percent)
```

```{r city_swap}
ide <- ide %>%
  rename(from_city_raw = from_city) %>% 
  left_join(
    y = zipcodes,
    by = c(
      "from_state_norm" = "state",
      "from_zip_norm" = "zip"
    )
  ) %>% 
  rename(city_match = city) %>% 
  mutate(
    match_abb = is_abbrev(from_city_norm, city_match),
    match_dist = str_dist(from_city_norm, city_match),
    from_city_swap = if_else(
      condition = match_abb | match_dist == 1,
      true = city_match,
      false = from_city_norm
    )
  ) %>% 
  select(
    -match_abb,
    -match_dist,
    -city_match
  )
```

```{r}
ide <- ide %>%
  rename(to_city_raw = to_city) %>% 
  left_join(
    y = zipcodes,
    by = c(
      "to_state" = "state",
      "to_zip_norm" = "zip"
    )
  ) %>% 
  rename(city_match = city) %>% 
  mutate(
    match_abb = is_abbrev(to_city_norm, city_match),
    match_dist = str_dist(to_city_norm, city_match),
    to_city_swap = if_else(
      condition = match_abb | match_dist == 1,
      true = city_match,
      false = to_city_norm
    )
  ) %>% 
  select(
    -match_abb,
    -match_dist,
    -city_match
  )
```


```{r city_progress}
progress_table(
  ide$from_city_raw,
  ide$from_city_norm,
  ide$from_city_swap,
  ide$to_city_raw,
  ide$to_city_norm,
  ide$to_city_swap,
  compare = valid_place
)
```
## Conclude

```{r clean-glimpse}
glimpse(sample_n(ide, 1000))
```

1. There are `r comma(nrow(ide))` records in the database.
1. There are no duplicate records in the database.
1. The range and distribution of `amount` and `date` seem reasonable.
1. There are `r comma(sum(ide$na_flag))` records missing key variables.
1. Consistency in geographic data has been improved with `campfin::normal_*()`.
1. The 4-digit `year` variable has been created with `lubridate::year()`.

## Export

```{r}
ide <- ide %>% 
  rename_all(~str_replace(., "_swap", "_clean")) %>% 
  select(-ends_with("city_norm")) %>% 
  rename_all(~str_replace(., "_norm", "_clean")) %>% 
  rename_all(~str_remove(., "_raw"))
```


Now the file can be saved on disk for upload to the Accountability server. We
will name the object using a date range of the records included.

```{r clean-timestamp}
ide$date <- as.Date(ide$date)
min_dt <- str_remove_all(min(ide$date, na.rm = TRUE), "-")
max_dt <- str_remove_all(max(ide$date, na.rm = TRUE), "-")
csv_ts <- paste(min_dt, max_dt, sep = "-")
```

```{r clean-dir}
clean_dir <- dir_create(here("state","id", "expends", "data", "clean"))
clean_csv <- path(clean_dir, glue("id_expends_{csv_ts}.csv"))
clean_rds <- path_ext_set(clean_csv, "rds")
basename(clean_csv)
```

```{r clean-write}
write_csv(ide, clean_csv, na = "")
write_rds(ide, clean_rds, compress = "xz")
(clean_size <- file_size(clean_csv))
```

## Upload

We can use the `aws.s3::put_object()` to upload the text file to the IRW server.

```{r aws-upload, eval=FALSE}
aws_key <- path("csv", basename(clean_csv))
if (!object_exists(aws_key, "publicaccountability")) {
  put_object(
    file = clean_csv,
    object = aws_key, 
    bucket = "publicaccountability",
    acl = "public-read",
    show_progress = TRUE,
    multipart = TRUE
  )
}
aws_head <- head_object(aws_key, "publicaccountability")
(aws_size <- as_fs_bytes(attr(aws_head, "content-length")))
unname(aws_size == clean_size)
```
